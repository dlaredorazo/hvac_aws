{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Amazon Timestream With SageMaker\n",
    "This is a sample notebook that lets Sagemaker integrate machine learning models\n",
    "with Amazon Timestream.\n",
    "\n",
    "In this notebook, we will use some sample Timestream queries to visualize data, \n",
    "visualize Anamoly scores, \n",
    "train a Random Cut Forest (RCF) model using the CPU utilization history etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "import timestreamquery as timestream\n",
    "from IPython import display\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import botocore\n",
    "import sagemaker\n",
    "import sys\n",
    "\n",
    "#################################################\n",
    "##### Timestream Configurations.  ###############\n",
    "#################################################\n",
    "ENDPOINT = \"us-east-1\" # <--- specify the region service endpoint\n",
    "PROFILE = \"default\" # <--- specify the AWS credentials profile\n",
    "DB_NAME = \"testdb\" # <--- specify the database created in Amazon Timestream\n",
    "TABLE_NAME = \"testTable\" # <--- specify the table created in Amazon Timestream\n",
    "\n",
    "client = timestream.createQueryClient(ENDPOINT, profile=PROFILE)\n",
    "\n",
    "#################################################\n",
    "##### SageMaker Configurations. #################\n",
    "#################################################\n",
    "\n",
    "bucket = 'ts-sagemaker'   # <--- specify a bucket you have access to\n",
    "prefix = 'customer_demo/rcf-benchmarks'\n",
    "execution_role = sagemaker.get_execution_role()\n",
    "    \n",
    "# check if the bucket exists\n",
    "try:\n",
    "    boto3.Session().client('s3').head_bucket(Bucket=bucket)\n",
    "except botocore.exceptions.ParamValidationError as e:\n",
    "    print('You either forgot to specify your S3 bucket'\n",
    "          ' or you gave your bucket an invalid name!')\n",
    "except botocore.exceptions.ClientError as e:\n",
    "    if e.response['Error']['Code'] == '403':\n",
    "        print(\"You don't have permission to access the bucket, {}.\".format(bucket))\n",
    "    elif e.response['Error']['Code'] == '404':\n",
    "        print(\"Your bucket, {}, doesn't exist!\".format(bucket))\n",
    "    else:\n",
    "        raise\n",
    "else:\n",
    "    print('Training input/output will be stored in: s3://{}/{}'.format(bucket, prefix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A helper function to visualize Anomaly scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualizeDataAndAnomalyScores(data, primary, secondary, primaryLabel = 'CPU User', secondaryLabel = 'Anomaly Score'):\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax2 = ax1.twinx()\n",
    "\n",
    "    ax1.plot(data[primary], color='C0', alpha=0.8)\n",
    "    ax2.plot(data[secondary], color='C1')\n",
    "\n",
    "    ax1.grid(which='major', axis='both')\n",
    "\n",
    "    ax1.set_ylabel(primaryLabel, color='C0')\n",
    "    ax2.set_ylabel(secondaryLabel, color='C1')\n",
    "\n",
    "    ax1.tick_params('y', colors='C0')\n",
    "    ax2.tick_params('y', colors='C1')\n",
    "\n",
    "    ax1.set_ylim(0, 100)\n",
    "    ax2.set_ylim(0, 10)\n",
    "    fig.set_figwidth(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logical view of data ##\n",
    "```sql\n",
    "DESCRIBE <db_name>.<table_name>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "DESCRIBE {}.{}\n",
    "\"\"\".format(DB_NAME, TABLE_NAME)\n",
    "\n",
    "result = timestream.executeQueryAndReturnAsDataframe(client, query, True)\n",
    "\n",
    "display.display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw query: Visualize the raw data, schema.\n",
    "\n",
    "```sql\n",
    "SELECT *\n",
    "FROM <db_name>.<table_name> \n",
    "WHERE --Filters\n",
    "    measure_name = 'cpu_user' AND     -- Measure being queries\n",
    "    time > ago(15m)                   -- Time predicate - Getting last 10 min of data\n",
    "LIMIT 20\n",
    "```\n",
    "\n",
    "* We can run queries at nanosecond level precision\n",
    "* The query is operating on raw data, rather than any aggregated view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT *\n",
    "FROM {}.{} \n",
    "WHERE measure_name = 'cpu_user' \n",
    "    AND time > ago(15m)\n",
    "LIMIT 10\n",
    "\"\"\".format(DB_NAME, TABLE_NAME)\n",
    "\n",
    "result = timestream.executeQueryAndReturnAsDataframe(client, query, True)\n",
    "\n",
    "display.display(result)\n",
    "\n",
    "REGION = result['region'][0]\n",
    "AZ = result['availability_zone'][0]\n",
    "MICROSERVICE_NAME = result['microservice_name'][0]\n",
    "INSTANCE_NAME = result['instance_name'][0]\n",
    "CELL = result['cell'][0]\n",
    "SILO = result['silo'][0]\n",
    "INSTANCE_TYPE = result['instance_type'][0]\n",
    "OS_VERSION = result['os_version'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch the CPU utilization of a host for the specified duration in the past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchCpuUserTimeseries(duration, region, cell, silo, az, microservice_name, instance_name):\n",
    "    query = \"\"\"\n",
    "SELECT BIN(time, 1m) AS time, ROUND(AVG(measure_value::double), 3) AS cpu_user\n",
    "FROM {0}.{1} \n",
    "WHERE measure_name = 'cpu_user' \n",
    "    AND time > ago({2})\n",
    "    AND region = '{3}' AND cell = '{4}' AND silo = '{5}' \n",
    "    AND availability_zone = '{6}' AND microservice_name = '{7}' \n",
    "    AND instance_name = '{8}'\n",
    "GROUP BY BIN(time, 1m)\n",
    "ORDER BY time ASC\n",
    "\"\"\".format(DB_NAME, TABLE_NAME, duration, region, cell, silo, az, microservice_name, instance_name)\n",
    "\n",
    "    cpu_user = timestream.executeQueryAndReturnAsDataframe(client, query)\n",
    "    cpu_user['cpu_user'] = pd.to_numeric(cpu_user['cpu_user'])\n",
    "    \n",
    "    return cpu_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage Timestream to find hosts with average CPU utilization across the fleet\n",
    "\n",
    "Leverage Timestream's sophisticated analytics functionality and expressive query language to find a set of hosts whose CPU utilization is representative of the Average host in the fleet.\n",
    "\n",
    "We use the average CPU utilization as baseline which we train on and identify expected utilization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DURATION = \"1h\"\n",
    "query = \"\"\"\n",
    "WITH region_avg AS \n",
    "(\n",
    "    SELECT avg(measure_value::double) AS threshold \n",
    "    FROM {0}.{1} \n",
    "    WHERE region = '{3}' AND  \n",
    "        measure_name = 'cpu_user' AND\n",
    "        time > ago({2})\n",
    "), \n",
    "host_avg AS \n",
    "(\n",
    "    SELECT instance_name, avg(measure_value::double) AS average_value \n",
    "    FROM {0}.{1} \n",
    "    WHERE region = '{3}' AND cell = '{4}' AND silo = '{5}' AND\n",
    "        availability_zone = '{6}' AND microservice_name = '{7}' AND\n",
    "        measure_name = 'cpu_user' AND\n",
    "        time > ago({2}) \n",
    "    GROUP BY instance_name\n",
    ") \n",
    "\n",
    "SELECT host_avg.instance_name AS high_cpu_hosts, \n",
    "       host_avg.average_value AS inst_avg, \n",
    "       region_avg.threshold AS region_avg_threshold\n",
    "FROM region_avg, host_avg \n",
    "WHERE host_avg.average_value BETWEEN (0.95 * region_avg.threshold) AND (1.05 * region_avg.threshold)\n",
    "LIMIT 10\n",
    "\"\"\".format(DB_NAME, TABLE_NAME, DURATION, REGION, CELL, SILO, AZ, MICROSERVICE_NAME)\n",
    "\n",
    "result = timestream.executeQueryAndReturnAsDataframe(client, query, True)\n",
    "display.display(result)\n",
    "\n",
    "AVG_CPU_HOST1 = result['high_cpu_hosts'][0]\n",
    "AVG_CPU_HOST2 = result['high_cpu_hosts'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the CPU utilization of one of the host for the past several days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DURATION = '7d'  ## Fetch CPU utilization for the past 7 days.\n",
    "\n",
    "cpu_user = fetchCpuUserTimeseries(DURATION, REGION, CELL, SILO, AZ, MICROSERVICE_NAME, INSTANCE_NAME)\n",
    "\n",
    "display.display(cpu_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(cpu_user['cpu_user'], color='C0', alpha=0.8)\n",
    "ax1.grid(which='major')\n",
    "ax1.set_ylabel('CPU User', color='C0')\n",
    "\n",
    "ax1.tick_params('y', colors='C0')\n",
    "\n",
    "ax1.set_ylim(0, 100)\n",
    "fig.set_figwidth(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a Random Cut Forest (RCF) model using the CPU utilization history.\n",
    "\n",
    "Amazon SageMaker Random Cut Forest (RCF) is an unsupervised algorithm for detecting anomalous data points within a data set. These are observations which diverge from otherwise well-structured or patterned data. Anomalies can manifest as unexpected spikes in time series data, breaks in periodicity, or unclassifiable data points. \n",
    "\n",
    "Additional details about the random cut forestt algorithm can be found in the [RCF Algorithm documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/randomcutforest.html).\n",
    "\n",
    "Particular to a SageMaker RCF training job are the following hyperparameters:\n",
    "\n",
    "* **num_samples_per_tree** - the number randomly sampled data points sent to each tree. As a general rule, 1/num_samples_per_tree should approximate the the estimated ratio of anomalies to normal points in the dataset.\n",
    "* **num_trees** - the number of trees to create in the forest. Each tree learns a separate model from different  samples of data. The full forest model uses the mean predicted anomaly score from each constituent tree.\n",
    "* **feature_dim** - the dimension of each data point.\n",
    "\n",
    "In addition to these RCF model hyperparameters, we provide additional parameters defining things like the EC2 instance type on which training will run, the S3 bucket containing the data, and the AWS access role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import RandomCutForest\n",
    "\n",
    "rcf = RandomCutForest(role=execution_role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.m4.xlarge',\n",
    "                      data_location='s3://{}/{}/'.format(bucket, prefix),\n",
    "                      output_path='s3://{}/{}/output'.format(bucket, prefix),\n",
    "                      num_samples_per_tree=512,\n",
    "                      num_trees=50)\n",
    "\n",
    "# automatically upload the training data to S3 and run the training job\n",
    "rcf.fit(rcf.record_set(cpu_user.cpu_user.values.reshape(-1,1)))\n",
    "\n",
    "print('Training job name: {}'.format(rcf.latest_training_job.job_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model for inference\n",
    "We create an inference endpoint using the SageMaker Python SDK deploy() function from the job we defined above. We specify the instance type where inference is computed as well as an initial number of instances to spin up. We recommend using the ml.c5 instance type as it provides the fastest inference time at the lowest cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcf_inference = rcf.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    ")\n",
    "\n",
    "print('Endpoint name: {}'.format(rcf_inference.endpoint))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the inference and perform sample inference\n",
    "\n",
    "We can pass data in a variety of formats to our inference endpoint. In this example we will demonstrate passing CSV-formatted data. Other available formats are JSON-formatted and RecordIO Protobuf. We make use of the SageMaker Python SDK utilities ``csv_serializer`` and ``json_deserializer`` when configuring the inference endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "rcf_inference.content_type = 'text/csv'\n",
    "rcf_inference.serializer = csv_serializer\n",
    "rcf_inference.accept = 'application/json'\n",
    "rcf_inference.deserializer = json_deserializer\n",
    "\n",
    "cpu_user_reshaped = cpu_user.cpu_user.values.reshape(-1,1)\n",
    "results = rcf_inference.predict(cpu_user_reshaped[:6])\n",
    "\n",
    "display.display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the Anomaly Scores for Data the model Trained on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rcf_inference.predict(cpu_user_reshaped)\n",
    "scores = [datum['score'] for datum in results['scores']]\n",
    "\n",
    "# add scores to cpu_user frame and print first few values\n",
    "cpu_user['score'] = pd.Series(scores, index=cpu_user.index)\n",
    "\n",
    "#display.display(cpu_user)\n",
    "cpu_user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data along with Anomaly Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = 0, len(cpu_user)\n",
    "\n",
    "visualizeDataAndAnomalyScores(cpu_user[start:end], 'cpu_user', 'score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CPU utilization of a host with similar profile\n",
    "\n",
    "Use the other host obtained earlier in the run, obtain its CPU utilization and then compute the Anomaly score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DURATION = '1d'  ## Fetch CPU utilization for the past 1 day.\n",
    "\n",
    "avg_cpu_user = fetchCpuUserTimeseries(DURATION, REGION, CELL, SILO, AZ, MICROSERVICE_NAME, AVG_CPU_HOST2)\n",
    "\n",
    "display.display(avg_cpu_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Anomaly Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rcf_inference.predict(avg_cpu_user.cpu_user.values.reshape(-1,1))\n",
    "scores = [datum['score'] for datum in results['scores']]\n",
    "\n",
    "# add scores to cpu_user frame and print first few values\n",
    "avg_cpu_user['score'] = pd.Series(scores, index=avg_cpu_user.index)\n",
    "\n",
    "avg_cpu_user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data and Anomaly Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = 0, len(cpu_user)\n",
    "\n",
    "visualizeDataAndAnomalyScores(avg_cpu_user[start:end], 'cpu_user', 'score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage Timestream to find a high CPU utilization host\n",
    "\n",
    "Leverage Timestream's sophisticated analytics functionality and expressive query language to find a set of high CPU utilization hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find High CPU hosts.\n",
    "DURATION = \"1h\"\n",
    "query = \"\"\"\n",
    "WITH region_avg AS \n",
    "(\n",
    "    SELECT avg(measure_value::double) AS threshold \n",
    "    FROM {0}.{1} \n",
    "    WHERE region = '{3}' AND  \n",
    "        measure_name = 'cpu_user' AND\n",
    "        time > ago({2})\n",
    "), \n",
    "host_avg AS \n",
    "(\n",
    "    SELECT region, cell, silo, availability_zone, microservice_name, instance_name, \n",
    "        avg(measure_value::double) AS average_value \n",
    "    FROM {0}.{1} \n",
    "    WHERE region = '{3}' AND\n",
    "        measure_name = 'cpu_user' AND\n",
    "        time > ago({2}) \n",
    "    GROUP BY region, cell, silo, availability_zone, microservice_name, instance_name\n",
    ") \n",
    "\n",
    "SELECT region, cell, silo, availability_zone, microservice_name,\n",
    "    host_avg.instance_name AS high_cpu_hosts, \n",
    "    host_avg.average_value AS inst_avg, \n",
    "    region_avg.threshold AS region_avg_threshold\n",
    "FROM region_avg, host_avg \n",
    "WHERE host_avg.average_value > (1.1 * region_avg.threshold)\n",
    "LIMIT 10\n",
    "\"\"\".format(DB_NAME, TABLE_NAME, DURATION, REGION)\n",
    "\n",
    "result = timestream.executeQueryAndReturnAsDataframe(client, query)\n",
    "display.display(result)\n",
    "\n",
    "HIGH_CPU_HOST = result['high_cpu_hosts'][0]\n",
    "REGION = result['region'][0]\n",
    "AZ = result['availability_zone'][0]\n",
    "MICROSERVICE_NAME = result['microservice_name'][0]\n",
    "CELL = result['cell'][0]\n",
    "SILO = result['silo'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get CPU utilization of a high CPU hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DURATION = '1d'\n",
    "high_cpu_user = fetchCpuUserTimeseries(DURATION, REGION, CELL, SILO, AZ, MICROSERVICE_NAME, HIGH_CPU_HOST)\n",
    "\n",
    "display.display(high_cpu_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Anomaly scores for this host's CPU utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_cpu_user_reshaped = high_cpu_user.cpu_user.values.reshape(-1,1)\n",
    "\n",
    "results = rcf_inference.predict(high_cpu_user_reshaped)\n",
    "high_cpu_scores = [datum['score'] for datum in results['scores']]\n",
    "\n",
    "# add scores to taxi data frame and print first few values\n",
    "high_cpu_user['score'] = pd.Series(high_cpu_scores, index=high_cpu_user.index)\n",
    "\n",
    "#display.display(high_cpu_user)\n",
    "high_cpu_user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data along with the high CPU utilization hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = 0, len(cpu_user)\n",
    "\n",
    "visualizeDataAndAnomalyScores(high_cpu_user[start:end], 'cpu_user', 'score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find hosts with Low CPU utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DURATION = \"1h\"\n",
    "query = \"\"\"\n",
    "WITH region_avg AS \n",
    "(\n",
    "    SELECT avg(measure_value::double) AS threshold \n",
    "    FROM {0}.{1} \n",
    "    WHERE region = '{3}' AND  \n",
    "        measure_name = 'cpu_user' AND\n",
    "        time > ago({2})\n",
    "), \n",
    "host_avg AS \n",
    "(\n",
    "    SELECT region, cell, silo, availability_zone, microservice_name, instance_name, \n",
    "        avg(measure_value::double) AS average_value \n",
    "    FROM {0}.{1} \n",
    "    WHERE region = '{3}' AND\n",
    "        measure_name = 'cpu_user' AND\n",
    "        time > ago({2}) \n",
    "    GROUP BY region, cell, silo, availability_zone, microservice_name, instance_name\n",
    ") \n",
    "\n",
    "SELECT region, cell, silo, availability_zone, microservice_name,\n",
    "    host_avg.instance_name AS low_cpu_hosts, \n",
    "    host_avg.average_value AS inst_avg, \n",
    "    region_avg.threshold AS region_avg_threshold\n",
    "FROM region_avg, host_avg \n",
    "WHERE host_avg.average_value < (0.9 * region_avg.threshold)\n",
    "LIMIT 10\n",
    "\"\"\".format(DB_NAME, TABLE_NAME, DURATION, REGION)\n",
    "\n",
    "result = timestream.executeQueryAndReturnAsDataframe(client, query, True)\n",
    "display.display(result)\n",
    "\n",
    "LOW_CPU_HOST = result['low_cpu_hosts'][0]\n",
    "REGION = result['region'][0]\n",
    "AZ = result['availability_zone'][0]\n",
    "MICROSERVICE_NAME = result['microservice_name'][0]\n",
    "CELL = result['cell'][0]\n",
    "SILO = result['silo'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the CPU utilization for the host with Low CPU utilization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DURATION = '1d'\n",
    "low_cpu_user = fetchCpuUserTimeseries(DURATION, REGION, CELL, SILO, AZ, MICROSERVICE_NAME, LOW_CPU_HOST)\n",
    "\n",
    "display.display(low_cpu_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute the Anomaly Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_cpu_user_reshaped = low_cpu_user.cpu_user.values.reshape(-1,1)\n",
    "\n",
    "results = rcf_inference.predict(low_cpu_user_reshaped)\n",
    "low_cpu_scores = [datum['score'] for datum in results['scores']]\n",
    "\n",
    "low_cpu_user['score'] = pd.Series(low_cpu_scores, index=low_cpu_user.index)\n",
    "\n",
    "low_cpu_user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data and Anomaly scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start, end = 0, len(low_cpu_user) \n",
    "low_cpu_user_subset = low_cpu_user[start:end]\n",
    "\n",
    "visualizeDataAndAnomalyScores(low_cpu_user[start:end], 'cpu_user', 'score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find anomalous hosts\n",
    "\n",
    "We now use the model to identify hosts with anomalous CPU utilization.\n",
    "* For each host, we obtain the time series of their per minute average CPU utilization.\n",
    "* Use the model to compute the anomaly score for the CPU utilization time series.\n",
    "* Identify hosts that have a high anomaly score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DURATION = '1h'\n",
    "query = \"\"\"\n",
    "WITH binned_cpu AS (\n",
    "    SELECT region, cell, silo, availability_zone, microservice_name, instance_name, \n",
    "        bin(time, 1m) AS time_bin, ROUND(avg(measure_value::double), 3) AS cpu_user\n",
    "    FROM {0}.{1}\n",
    "    WHERE time > ago({2})\n",
    "        AND measure_name = 'cpu_user'\n",
    "        AND region = '{3}' AND cell = '{4}'\n",
    "    GROUP BY region, cell, silo, availability_zone, microservice_name, instance_name, bin(time, 1m)\n",
    ")\n",
    "SELECT region, cell, silo, availability_zone, microservice_name, instance_name, CREATE_TIME_SERIES(time_bin, cpu_user) AS cpu_user_ts\n",
    "FROM binned_cpu\n",
    "GROUP BY region, cell, silo, availability_zone, microservice_name, instance_name\n",
    "\"\"\".format(DB_NAME, TABLE_NAME, DURATION, REGION, CELL)\n",
    "\n",
    "per_host_timeseries = timestream.executeQueryAndReturnAsDataframe(client, query, True)\n",
    "\n",
    "display.display(per_host_timeseries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "ANOMALY_THRESHOLD = 4\n",
    "\n",
    "anomalous_hosts = list()\n",
    "for item in per_host_timeseries.itertuples():\n",
    "    # Reshape the data as sought by the model\n",
    "    cpu_util = np.reshape([[x['value']] for x in item.cpu_user_ts], (-1,1))\n",
    "    \n",
    "    # Invoke inference to get the anomaly score.\n",
    "    results = rcf_inference.predict(cpu_util)\n",
    "    result_cpu_scores = [datum['score'] for datum in results['scores']]\n",
    "    \n",
    "    # Identify anomalous hosts by averaging the anomaly scores for all measurements\n",
    "    # and checking if it is greater than a threshold.\n",
    "    if np.average(result_cpu_scores) > ANOMALY_THRESHOLD:\n",
    "        anomalous_hosts.append(item)\n",
    "        \n",
    "print(\"Fractional anomalous hosts: {}\".format(round(len(anomalous_hosts)/len(per_host_timeseries.index), 3)))\n",
    "for item in anomalous_hosts:\n",
    "    print(\"[Region: {}, Cell: {}, Silo: {}, Microservice: {}, Instance name: {}]. Avg CPU: {}\".format(\n",
    "        item.region, item.cell, item.silo, item.microservice_name, item.instance_name,\n",
    "        round(np.average([y['value'] for y in item.cpu_user_ts])), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop and Delete the Endpoint\n",
    "Finally, we should delete the endpoint before we close the notebook.\n",
    "\n",
    "To do so execute the cell below. Alternately, you can navigate to the \"Endpoints\" tab in the SageMaker console, select the endpoint with the name stored in the variable endpoint_name, and select \"Delete\" from the \"Actions\" dropdown menu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker.Session().delete_endpoint(rcf_inference.endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other models relevant to time series\n",
    "\n",
    "* [**DeepAR Forecasting Algorithm**](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar.html)\n",
    "    * Example Notebooks\n",
    "        * [Timeseries forecasting with DeepAR - Synthetic data](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/deepar_synthetic/deepar_synthetic.ipynb)\n",
    "        * [SageMaker/DeepAR demo on electricity dataset](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/deepar_electricity/DeepAR-Electricity.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}