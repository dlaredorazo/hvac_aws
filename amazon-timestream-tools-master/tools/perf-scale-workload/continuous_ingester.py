from collections import namedtuple
import threading
import multiprocessing
import time
from tdigest import TDigest
import timestreamwrite as tswrite
import model
import datetime
from timeit import default_timer as timer
import json
import sys, traceback
import random
import math
import signal

def getTimestampMillis():
    ## Pick a random timestamp in the 200 minute window to stagger data points generated by
    ## the different processes.
    currentTime = int(round(time.time() * 1000))
    return random.randint(currentTime - 100, currentTime + 100)

def getCurrentTimestampMillis():
    return int(round(time.time() * 1000))

### State shared by the threads within tne same process. Since processes have different global
### state, each process will have it's own local copy of this.
seriesId = 0
timestamp = getTimestampMillis()
sigInt = False
lock = threading.Lock()
event = threading.Event()

def signalHandler(sig, frame):
    global sigInt
    global lock
    global event

    with lock:
        sigInt = True
        event.set()

#########################################
######### Ingestion Thread ##############
#########################################
class IngestionThread(threading.Thread):
    def __init__(self, threadId, args, dimensionMetrics, dimensionEvents, highUtilizationHosts, lowUtilizationHosts, event):
        threading.Thread.__init__(self)
        self.threadId = threadId
        self.args = args
        self.dimensionMetrics = dimensionMetrics
        self.dimensionEvents = dimensionEvents
        self.client = tswrite.createWriteClient(args.endpoint, profile=args.profile)
        self.databaseName = args.databaseName
        self.tableName = args.tableName
        self.numMetrics = len(dimensionMetrics)
        self.numEvents = len(dimensionEvents)
        self.digest = TDigest()  ## Use the t-digest to compute the streaming percentiles
        self.count = 0
        self.success = 0
        self.sum = 0.0
        self.variance = float('nan')
        self.highUtilizationHosts = highUtilizationHosts
        self.lowUtilizationHosts = lowUtilizationHosts
        self.sigInt = False
        self.event = event

    def run(self):
        global seriesId
        global timestamp
        global lock

        idx = 0
        mean = 0.0
        squared = 0.0

        while True:
            with lock:
                if self.sigInt == True or sigInt == True or self.event.is_set():
                    print("Thread {} exiting.".format(self.threadId))
                    break

                seriesId += 1
                if seriesId >= self.numMetrics + self.numEvents:
                    ## Wrapping around, so move to new timestamp.
                    seriesId = 0
                    newTimestamp = timestamp + self.args.intervalMillis
                    currentTime = getCurrentTimestampMillis()
                    ## Check if the timestamps are falling behind
                    if newTimestamp < currentTime - 0.05 * self.args.intervalMillis:
                        print("Can't keep up ingestion to the desired inter-event interval. Expected interval: {} ms. Actual: {} ms. Consider increasing concurrency or processes.".format(self.args.intervalMillis, currentTime - timestamp))
                        ## Move time forward.
                        timestamp = getTimestampMillis()
                    else:
                        timestamp = newTimestamp
                        ## Check if we are ingesting too fast, then slow down.
                        if timestamp > currentTime - 1000:
                            ## Slow down
                            sleepTimeSecs = int((timestamp - currentTime)/1000)
                            print("Thread {} sleeping for {} secs".format(self.threadId, sleepTimeSecs))
                            time.sleep(sleepTimeSecs)

                    now = datetime.datetime.now()
                    print("Resetting to first series from thread: [{}] at time {}. Timestamp set to: {}.".format(self.threadId, now.strftime("%Y-%m-%d %H:%M:%S"), timestamp))

                localSeriesId = seriesId
                localTimestamp = timestamp

            if localSeriesId < self.numMetrics:
                commonAttributes = model.createWriteRecordCommonAttributes(self.dimensionMetrics[localSeriesId])
                records = model.createRandomMetrics(seriesId, localTimestamp, "MILLISECONDS", self.highUtilizationHosts, self.lowUtilizationHosts)
            else:
                commonAttributes = model.createWriteRecordCommonAttributes(self.dimensionEvents[localSeriesId - self.numMetrics])
                records = model.createRandomEvent(localTimestamp, "MILLISECONDS")

            idx += 1
            start = timer()
            try:
                writeResult = tswrite.writeRecords(self.client, self.databaseName, self.tableName, commonAttributes, records)
                self.success += 1
            except Exception as e:
                print(e)
                exc_type, exc_value, exc_traceback = sys.exc_info()
                traceback.print_exception(exc_type, exc_value, exc_traceback, limit=2, file=sys.stdout)
                requestId = "RequestId: {}".format(e.response['ResponseMetadata']['RequestId'])
                print(requestId)
                print(json.dumps(commonAttributes, indent=2))
                print(json.dumps(records, indent=2))
                continue
            finally:
                self.count += 1
                end = timer()
                cur = end - start
                self.digest.update(cur)
                self.sum += cur
                ## Computing the streaming M^2 (squared distance from mean)
                delta = cur - mean
                mean += delta / self.count
                squared += delta * (cur - mean)
                if self.count > 1:
                    self.variance = float(squared / (self.count - 1))

            requestId = writeResult['ResponseMetadata']['RequestId']
            if idx % 1000 == 0:
                now = datetime.datetime.now()
                print("{}. {}. {}. Last RequestId: {}. Avg={:,}, Stddev={:,}, 50thPerc={:,}, 90thPerc={:,}, 99thPerc={:,}".format(
                    self.threadId, idx, now.strftime("%Y-%m-%d %H:%M:%S"), requestId, round(self.sum / self.count, 3),
                    round(math.sqrt(self.variance), 3), round(self.digest.percentile(50), 3),
                    round(self.digest.percentile(90), 3), round(self.digest.percentile(99), 3)))

    def interrupt(self):
        print("Interrupting thread: ", self.threadId)
        self.sigInt = True

IngestionSummaryStats = namedtuple('IngestionSummaryStats', 'digest count success sum variance')

#########################################
## Process that spawns multiple        ##
## ingestion threads.                  ##
#########################################
class MultiProcessIngestWorker(multiprocessing.Process):
    def __init__(self, processId, args, dimensionMetrics, dimensionEvents, highUtilizationHosts, lowUtilizationHosts, conn, event):
        super(MultiProcessIngestWorker, self).__init__()
        self.processId = processId
        self.args = args
        self.conn = conn
        self.event = event
        self.dimensionMetrics = dimensionMetrics
        self.dimensionEvents = dimensionEvents
        self.highUtilizationHosts = highUtilizationHosts
        self.lowUtilizationHosts = lowUtilizationHosts
        self.threads = list()

    def run(self):
        global lock
        global seriesId
        global timestamp

        with lock:
            ## Randomly pick a series ID to start for this process.
            seriesId = random.randint(0, len(self.dimensionEvents) + len(self.dimensionMetrics) - 1)
            timestamp = getTimestampMillis()
            print("Process {} using start series ID: {}".format(self.processId, seriesId))

        ## Register sigint handler
        signal.signal(signal.SIGINT, signalHandler)
        overallSummary = None
        ingestionStart = timer()

        try:
            for threadId in range(self.args.concurrency):
                threadIdStr = "{}-{}".format(self.processId, threadId + 1)
                print("Starting ThreadId: {}".format(threadIdStr))
                thread = IngestionThread(threadIdStr, self.args, self.dimensionMetrics, self.dimensionEvents,
                    self.highUtilizationHosts, self.lowUtilizationHosts, self.event)
                thread.start()
                self.threads.append(thread)

            success = 0
            count = 0
            totalLatency = 0.0
            aggregatedDigests = TDigest()
            pooledVariance = 0.0
            for t in self.threads:
                t.join()
                success += t.success
                ## Pool the variance.
                if count == 0:
                    pooledVariance = t.variance
                else:
                    pooledVariance = ((count - 1) * pooledVariance + (t.count - 1) * t.variance) / ((count - 1) + (t.count - 1))
                count += t.count
                aggregatedDigests += t.digest
                totalLatency += t.sum

            print("[Process: {}] Total={:,}, Success={:,}, Avg={:,}, Stddev={:,}, 50thPerc={:,}, 90thPerc={:,}, 99thPerc={:,}".format(
                self.processId, count, success,
                round(totalLatency / count, 3),
                round(math.sqrt(pooledVariance), 3), round(aggregatedDigests.percentile(50), 3),
                round(aggregatedDigests.percentile(90), 3), round(aggregatedDigests.percentile(99), 3)))

            overallSummary = IngestionSummaryStats(aggregatedDigests, count, success, totalLatency, pooledVariance)
            ingestionEnd = timer()
            print("Total time to ingest: {:,} seconds".format(round(ingestionEnd - ingestionStart, 2)))
        finally:
            self.conn.send(overallSummary)


#########################################
######### Ingest load ###################
#########################################

## List of processes that are being created
processes = list()

def signalHandlerMultiProc(sig, frame):
    if len(processes) == 0:
        print("No processes to interrupt")

    for p, conn, event in processes:
        event.set()

def initializeHighAndLowUtilizationHosts(numHosts):
    hostIds = list(range(numHosts))
    utilizationRand = random.Random(12345)
    utilizationRand.shuffle(hostIds)
    lowUtilizationHosts = frozenset(hostIds[0:int(0.2 * len(hostIds))])
    highUtilizationHosts = frozenset(hostIds[-int(0.2 * len(hostIds)):])
    return (lowUtilizationHosts, highUtilizationHosts)

def ingestRecordsMultiProc(dimensionsMetrics, dimensionsEvents, args):
    ## Register sigint handler
    signal.signal(signal.SIGINT, signalHandlerMultiProc)

    numHosts = len(dimensionsMetrics)
    remainder = numHosts % args.processes
    startId = 0

    ingestionStart = timer()

    for processId in range(1, args.processes + 1):
        endId = startId + int(numHosts / args.processes) + (1 if remainder > 0 else 0)
        if endId > numHosts:
            print("Number of processes more than number of hosts, skipping process creation")
            break
        print("Starting process {} with host ranges: [{}, {}]".format(processId, startId, endId - 1))

        ## Select a subset of hosts
        dimensionsMetricsLocal = dimensionsMetrics[startId:endId]
        dimensionsMetricsSet = set()
        for dim in dimensionsMetricsLocal:
            dimensionsMetricsSet.add((dim.region, dim.cell, dim.silo, dim.availability_zone, dim.microservice_name, dim.instance_name))
        dimensionsEventsLocal = list()
        ## Select the dimension events for the hosts selected above.
        for dim in dimensionsEvents:
            host = (dim.region, dim.cell, dim.silo, dim.availability_zone, dim.microservice_name, dim.instance_name)
            if host in dimensionsMetricsSet:
                dimensionsEventsLocal.append(dim)

        print("Starting process {} with host ranges: [{}, {}]. Metrics: {}. Events: {}".format(processId, startId, endId - 1,
            len(dimensionsMetricsLocal), len(dimensionsEventsLocal)))
        lowUtilizationHosts, highUtilizationHosts = initializeHighAndLowUtilizationHosts(len(dimensionsMetricsLocal))
        parentConn, childConn = multiprocessing.Pipe()
        manager = multiprocessing.Manager()
        event = manager.Event()
        process = MultiProcessIngestWorker(processId, args, dimensionsMetricsLocal, dimensionsEventsLocal, highUtilizationHosts, lowUtilizationHosts, childConn, event)
        process.start()
        processes.append((process, parentConn, event))
        remainder -= 1
        startId = endId

    success = 0
    count = 0
    totalLatency = 0.0
    aggregatedDigests = TDigest()
    pooledVariance = 0.0
    for p, conn, event in processes:
        output = conn.recv()
        p.join()
        if output == None:
            continue

        success += output.success
        ## Pool the variance.
        if count == 0:
            pooledVariance = output.variance
        else:
            pooledVariance = ((count - 1) * pooledVariance + (output.count - 1) * output.variance) / ((count - 1) + (output.count - 1))
        count += output.count
        aggregatedDigests += output.digest
        totalLatency += output.sum

    print("[OVERALL] Total={:,}, Success={:,}, Avg={:,}, Stddev={:,}, 50thPerc={:,}, 90thPerc={:,}, 99thPerc={:,}".format(count, success,
        round(totalLatency / count, 3),
        round(math.sqrt(pooledVariance), 3), round(aggregatedDigests.percentile(50), 3),
        round(aggregatedDigests.percentile(90), 3), round(aggregatedDigests.percentile(99), 3)))

    ingestionEnd = timer()
    print("Total time to ingest: {:,} seconds".format(round(ingestionEnd - ingestionStart, 2)))
